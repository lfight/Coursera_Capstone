{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8+y38kZcR80/uiWttuwvE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lfight/Coursera_Capstone/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipkSdYLv9toM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "02412736-f2ac-470f-a2d3-3650a7697a6e"
      },
      "source": [
        "!git clone https://github.com/macanv/BERT-BiLSTM-CRF-NER.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BERT-BiLSTM-CRF-NER'...\n",
            "remote: Enumerating objects: 385, done.\u001b[K\n",
            "remote: Total 385 (delta 0), reused 0 (delta 0), pack-reused 385\u001b[K\n",
            "Receiving objects: 100% (385/385), 3.85 MiB | 13.10 MiB/s, done.\n",
            "Resolving deltas: 100% (211/211), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wngmFc113thI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "465e47d6-1626-4142-a59e-b164b86f3394"
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "os.chdir('/content/BERT-BiLSTM-CRF-NER')\n",
        "os.getcwd()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/BERT-BiLSTM-CRF-NER'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfdUJroh9-PG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "01a3c5df-68ce-4575-aac6-2ce07bf47389"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-25 14:14:06--  https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.145.128, 108.177.112.128, 172.217.212.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.145.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381892918 (364M) [application/zip]\n",
            "Saving to: ‘chinese_L-12_H-768_A-12.zip’\n",
            "\n",
            "chinese_L-12_H-768_ 100%[===================>] 364.20M  30.6MB/s    in 12s     \n",
            "\n",
            "2020-08-25 14:14:18 (30.6 MB/s) - ‘chinese_L-12_H-768_A-12.zip’ saved [381892918/381892918]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51i8B8vr-E40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "0b0eaa5a-23fc-4ef5-ee32-fdeb3fe898d7"
      },
      "source": [
        "!unzip chinese_L-12_H-768_A-12.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  chinese_L-12_H-768_A-12.zip\n",
            "   creating: chinese_L-12_H-768_A-12/\n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: chinese_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PblZFcC6WNO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "b28f313c-c698-40fa-c3b7-36d2639a3045"
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkUrN21W4E4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0e4e8c1-20f3-4678-b5ef-b61a965cda28"
      },
      "source": [
        "!python setup.py install"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing bert_base.egg-info/PKG-INFO\n",
            "writing dependency_links to bert_base.egg-info/dependency_links.txt\n",
            "writing entry points to bert_base.egg-info/entry_points.txt\n",
            "writing requirements to bert_base.egg-info/requires.txt\n",
            "writing top-level names to bert_base.egg-info/top_level.txt\n",
            "writing manifest file 'bert_base.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/bert_base\n",
            "creating build/bdist.linux-x86_64/egg/bert_base/runs\n",
            "copying build/lib/bert_base/runs/__init__.py -> build/bdist.linux-x86_64/egg/bert_base/runs\n",
            "creating build/bdist.linux-x86_64/egg/bert_base/train\n",
            "copying build/lib/bert_base/train/conlleval.py -> build/bdist.linux-x86_64/egg/bert_base/train\n",
            "copying build/lib/bert_base/train/models.py -> build/bdist.linux-x86_64/egg/bert_base/train\n",
            "copying build/lib/bert_base/train/__init__.py -> build/bdist.linux-x86_64/egg/bert_base/train\n",
            "copying build/lib/bert_base/train/tf_metrics.py -> build/bdist.linux-x86_64/egg/bert_base/train\n",
            "copying build/lib/bert_base/train/bert_lstm_ner.py -> build/bdist.linux-x86_64/egg/bert_base/train\n",
            "copying build/lib/bert_base/train/train_helper.py -> build/bdist.linux-x86_64/egg/bert_base/train\n",
            "copying build/lib/bert_base/train/lstm_crf_layer.py -> build/bdist.linux-x86_64/egg/bert_base/train\n",
            "creating build/bdist.linux-x86_64/egg/bert_base/client\n",
            "copying build/lib/bert_base/client/__init__.py -> build/bdist.linux-x86_64/egg/bert_base/client\n",
            "creating build/bdist.linux-x86_64/egg/bert_base/server\n",
            "copying build/lib/bert_base/server/http.py -> build/bdist.linux-x86_64/egg/bert_base/server\n",
            "copying build/lib/bert_base/server/simple_flask_http_service.py -> build/bdist.linux-x86_64/egg/bert_base/server\n",
            "copying build/lib/bert_base/server/zmq_decor.py -> build/bdist.linux-x86_64/egg/bert_base/server\n",
            "copying build/lib/bert_base/server/__init__.py -> build/bdist.linux-x86_64/egg/bert_base/server\n",
            "copying build/lib/bert_base/server/helper.py -> build/bdist.linux-x86_64/egg/bert_base/server\n",
            "copying build/lib/bert_base/server/graph.py -> build/bdist.linux-x86_64/egg/bert_base/server\n",
            "copying build/lib/bert_base/__init__.py -> build/bdist.linux-x86_64/egg/bert_base\n",
            "creating build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/modeling_test.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/tokenization_test.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/optimization.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/modeling.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/run_squad.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/run_classifier.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/extract_features.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/tokenization.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/create_pretraining_data.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/__init__.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/run_pretraining.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "copying build/lib/bert_base/bert/optimization_test.py -> build/bdist.linux-x86_64/egg/bert_base/bert\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/runs/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/train/conlleval.py to conlleval.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/train/models.py to models.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/train/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/train/tf_metrics.py to tf_metrics.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/train/bert_lstm_ner.py to bert_lstm_ner.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/train/train_helper.py to train_helper.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/train/lstm_crf_layer.py to lstm_crf_layer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/client/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/server/http.py to http.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/server/simple_flask_http_service.py to simple_flask_http_service.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/server/zmq_decor.py to zmq_decor.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/server/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/server/helper.py to helper.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/server/graph.py to graph.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/modeling_test.py to modeling_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/tokenization_test.py to tokenization_test.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/optimization.py to optimization.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/modeling.py to modeling.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/run_squad.py to run_squad.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/run_classifier.py to run_classifier.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/extract_features.py to extract_features.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/tokenization.py to tokenization.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/create_pretraining_data.py to create_pretraining_data.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/run_pretraining.py to run_pretraining.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/bert_base/bert/optimization_test.py to optimization_test.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bert_base.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bert_base.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bert_base.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bert_base.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bert_base.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bert_base.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying bert_base.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "creating 'dist/bert_base-0.0.9-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing bert_base-0.0.9-py3.6.egg\n",
            "removing '/usr/local/lib/python3.6/dist-packages/bert_base-0.0.9-py3.6.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.6/dist-packages/bert_base-0.0.9-py3.6.egg\n",
            "Extracting bert_base-0.0.9-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "bert-base 0.0.9 is already the active version in easy-install.pth\n",
            "Installing bert-base-ner-train script to /usr/local/bin\n",
            "Installing bert-base-serving-start script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/bert_base-0.0.9-py3.6.egg\n",
            "Processing dependencies for bert-base==0.0.9\n",
            "Searching for termcolor==1.1.0\n",
            "Best match: termcolor 1.1.0\n",
            "Adding termcolor 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for GPUtil==1.4.0\n",
            "Best match: GPUtil 1.4.0\n",
            "Processing GPUtil-1.4.0-py3.6.egg\n",
            "GPUtil 1.4.0 is already the active version in easy-install.pth\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/GPUtil-1.4.0-py3.6.egg\n",
            "Searching for pyzmq==19.0.2\n",
            "Best match: pyzmq 19.0.2\n",
            "Adding pyzmq 19.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.18.5\n",
            "Best match: numpy 1.18.5\n",
            "Adding numpy 1.18.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for bert-base==0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCQdbCuH63jD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "117af880-55d7-487c-df4f-7e848a0adc0b"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 59kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\u001b[0m\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.31.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.18.5)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (49.2.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.0)\n",
            "Installing collected packages: keras-applications, tensorboard, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzSLH5GQ6JJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "4d46e748-fd73-4e35-d3d9-9cf73191796f"
      },
      "source": [
        "!bert-base-ner-train -help"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "usage: bert-base-ner-train [-h] [-data_dir DATA_DIR]\n",
            "                           [-bert_config_file BERT_CONFIG_FILE]\n",
            "                           [-output_dir OUTPUT_DIR]\n",
            "                           [-init_checkpoint INIT_CHECKPOINT]\n",
            "                           [-vocab_file VOCAB_FILE]\n",
            "                           [-max_seq_length MAX_SEQ_LENGTH] [-do_train]\n",
            "                           [-do_eval] [-do_predict] [-batch_size BATCH_SIZE]\n",
            "                           [-learning_rate LEARNING_RATE]\n",
            "                           [-num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                           [-dropout_rate DROPOUT_RATE] [-clip CLIP]\n",
            "                           [-warmup_proportion WARMUP_PROPORTION]\n",
            "                           [-lstm_size LSTM_SIZE] [-num_layers NUM_LAYERS]\n",
            "                           [-cell CELL]\n",
            "                           [-save_checkpoints_steps SAVE_CHECKPOINTS_STEPS]\n",
            "                           [-save_summary_steps SAVE_SUMMARY_STEPS]\n",
            "                           [-filter_adam_var FILTER_ADAM_VAR]\n",
            "                           [-do_lower_case DO_LOWER_CASE] [-clean CLEAN]\n",
            "                           [-device_map DEVICE_MAP] [-label_list LABEL_LIST]\n",
            "                           [-verbose] [-ner NER] [-version]\n",
            "bert-base-ner-train: error: argument -h/--help: ignored explicit argument 'elp'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzlAhPiu6Uh5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b87b6a7-0d8f-4d0a-ee4e-110b93f130f1"
      },
      "source": [
        "!bert-base-ner-train -data_dir /content/sample_data -output_dir /content/sample_data/output -init_checkpoint chinese_L-12_H-768_A-12/bert_model.ckpt -bert_config_file chinese_L-12_H-768_A-12/bert_config.json -vocab_file chinese_L-12_H-768_A-12/vocab.txt -label_list 'B_M,I_M,B_S,I_S,B_W,I_W,B_R,I_R'"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "usage: /usr/local/bin/bert-base-ner-train -data_dir /content/sample_data -output_dir /content/sample_data/output -init_checkpoint chinese_L-12_H-768_A-12/bert_model.ckpt -bert_config_file chinese_L-12_H-768_A-12/bert_config.json -vocab_file chinese_L-12_H-768_A-12/vocab.txt -label_list B_M,I_M,B_S,I_S,B_W,I_W,B_R,I_R\n",
            "                 ARG   VALUE\n",
            "__________________________________________________\n",
            "          batch_size = 64\n",
            "    bert_config_file = chinese_L-12_H-768_A-12/bert_config.json\n",
            "                cell = lstm\n",
            "               clean = True\n",
            "                clip = 0.5\n",
            "            data_dir = /content/sample_data\n",
            "          device_map = 0\n",
            "             do_eval = True\n",
            "       do_lower_case = True\n",
            "          do_predict = True\n",
            "            do_train = True\n",
            "        dropout_rate = 0.5\n",
            "     filter_adam_var = False\n",
            "     init_checkpoint = chinese_L-12_H-768_A-12/bert_model.ckpt\n",
            "          label_list = B_M,I_M,B_S,I_S,B_W,I_W,B_R,I_R\n",
            "       learning_rate = 1e-05\n",
            "           lstm_size = 128\n",
            "      max_seq_length = 202\n",
            "                 ner = ner\n",
            "          num_layers = 1\n",
            "    num_train_epochs = 10\n",
            "          output_dir = /content/sample_data/output\n",
            "save_checkpoints_steps = 500\n",
            "  save_summary_steps = 500\n",
            "             verbose = False\n",
            "          vocab_file = chinese_L-12_H-768_A-12/vocab.txt\n",
            "   warmup_proportion = 0.1\n",
            "\n",
            "I:NER Training:[ber:tra:558]:***** Running training *****\n",
            "I:NER Training:[ber:tra:559]:  Num examples = 8000\n",
            "I:NER Training:[ber:tra:560]:  Batch size = 64\n",
            "I:NER Training:[ber:tra:561]:  Num steps = 1250\n",
            "I:NER Training:[ber:tra:566]:***** Running evaluation *****\n",
            "I:NER Training:[ber:tra:567]:  Num examples = 1499\n",
            "I:NER Training:[ber:tra:568]:  Batch size = 64\n",
            "I:NER Training:[ber:fil:301]:Writing example 0 of 8000\n",
            "I:NER Training:[ber:con:262]:*** Example ***\n",
            "I:NER Training:[ber:con:263]:guid: train-0\n",
            "I:NER Training:[ber:con:265]:tokens: 而 他 们 所 讨 论 的 就 是 佛 教 和 西 方 社 会 对 于 [UNK] 破 坏 性 的 情 感 [UNK] 和 近 几 年 在 美 国 非 常 流 行 的 [UNK] 情 绪 管 理 [UNK] 有 何 而 为 而 有 着 不 同 的 说 法 和 见 解 。\n",
            "I:NER Training:[ber:con:266]:input_ids: 101 5445 800 812 2792 6374 6389 4638 2218 3221 867 3136 1469 6205 3175 4852 833 2190 754 100 4788 1776 2595 4638 2658 2697 100 1469 6818 1126 2399 1762 5401 1744 7478 2382 3837 6121 4638 100 2658 5328 5052 4415 100 3300 862 5445 711 5445 3300 4708 679 1398 4638 6432 3791 1469 6224 6237 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:267]:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:268]:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:269]:label_ids: 11 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 9 6 8 8 8 8 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:262]:*** Example ***\n",
            "I:NER Training:[ber:con:263]:guid: train-1\n",
            "I:NER Training:[ber:con:265]:tokens: 例 如 ： 青 少 年 吸 烟 的 害 处 不 仅 是 他 本 身 还 会 对 社 会 的 未 来 发 展 。 因 为 青 少 年 是 未 来 社 会 的 主 要 发 展 因 原 之 一 。\n",
            "I:NER Training:[ber:con:266]:input_ids: 101 891 1963 8038 7471 2208 2399 1429 4170 4638 2154 1905 679 788 3221 800 3315 6716 6820 833 2190 4852 833 4638 3313 3341 1355 2245 511 1728 711 7471 2208 2399 3221 3313 3341 4852 833 4638 712 6206 1355 2245 1728 1333 722 671 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:267]:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:268]:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:269]:label_ids: 11 8 8 8 8 8 8 8 8 6 10 10 8 8 8 7 8 8 8 8 8 8 8 8 8 8 8 8 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 9 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:262]:*** Example ***\n",
            "I:NER Training:[ber:con:263]:guid: train-2\n",
            "I:NER Training:[ber:con:265]:tokens: 我 们 在 回 去 加 拿 大 之 前 ， 买 了 很 多 手 信 送 给 那 一 边 的 朋 友 。 我 的 这 一 个 假 期 ， 真 的 过 到 很 愉 快 呢 ！\n",
            "I:NER Training:[ber:con:266]:input_ids: 101 2769 812 1762 1726 1343 1217 2897 1920 722 1184 8024 743 749 2523 1914 2797 928 6843 5314 6929 671 6804 4638 3301 1351 511 2769 4638 6821 671 702 969 3309 8024 4696 4638 6814 1168 2523 2690 2571 1450 8013 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:267]:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:268]:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:269]:label_ids: 11 8 8 8 8 6 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:262]:*** Example ***\n",
            "I:NER Training:[ber:con:263]:guid: train-3\n",
            "I:NER Training:[ber:con:265]:tokens: 我 认 为 所 有 的 人 类 具 有 生 存 的 权 利 ， 而 且 人 类 具 有 幸 福 地 生 存 的 权 利 。 每 国 家 规 定 各 种 各 样 的 幸 福 的 权 利 。\n",
            "I:NER Training:[ber:con:266]:input_ids: 101 2769 6371 711 2792 3300 4638 782 5102 1072 3300 4495 2100 4638 3326 1164 8024 5445 684 782 5102 1072 3300 2401 4886 1765 4495 2100 4638 3326 1164 511 3680 1744 2157 6226 2137 1392 4905 1392 3416 4638 2401 4886 4638 3326 1164 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:267]:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:268]:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:269]:label_ids: 11 8 8 8 8 8 8 8 6 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:262]:*** Example ***\n",
            "I:NER Training:[ber:con:263]:guid: train-4\n",
            "I:NER Training:[ber:con:265]:tokens: 我 觉 得 这 个 俗 语 是 完 全 对 的 ， 每 个 人 的 平 常 的 生 活 、 行 为 都 是 可 以 从 他 家 庭 的 背 景 而 看 见 的 。\n",
            "I:NER Training:[ber:con:266]:input_ids: 101 2769 6230 2533 6821 702 921 6427 3221 2130 1059 2190 4638 8024 3680 702 782 4638 2398 2382 4638 4495 3833 510 6121 711 6963 3221 1377 809 794 800 2157 2431 4638 5520 3250 5445 4692 6224 4638 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:267]:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:268]:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:269]:label_ids: 11 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 9 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:fil:301]:Writing example 5000 of 8000\n",
            "I:NER Training:[ber:fil:301]:Writing example 0 of 1499\n",
            "I:NER Training:[ber:con:262]:*** Example ***\n",
            "I:NER Training:[ber:con:263]:guid: dev-0\n",
            "I:NER Training:[ber:con:265]:tokens: 所 以 她 常 常 被 她 父 母 挨 打 。 我 和 她 是 好 朋 友 ， 所 以 我 平 常 对 她 说 ： [UNK] 如 果 你 这 样 下 去 的 话 应 该 影 响 到 你 的 学 习 。\n",
            "I:NER Training:[ber:con:266]:input_ids: 101 2792 809 1961 2382 2382 6158 1961 4266 3678 2917 2802 511 2769 1469 1961 3221 1962 3301 1351 8024 2792 809 2769 2398 2382 2190 1961 6432 8038 100 1963 3362 872 6821 3416 678 1343 4638 6413 2418 6421 2512 1510 1168 872 4638 2110 739 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:267]:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:268]:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:269]:label_ids: 11 8 8 8 8 8 8 8 8 8 4 9 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 4 9 8 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:262]:*** Example ***\n",
            "I:NER Training:[ber:con:263]:guid: dev-1\n",
            "I:NER Training:[ber:con:265]:tokens: 我 估 计 各 国 家 的 这 种 措 施 越 来 越 多 ， 但 我 希 望 也 考 虑 吸 烟 人 的 利 益 。\n",
            "I:NER Training:[ber:con:266]:input_ids: 101 2769 844 6369 1392 1744 2157 4638 6821 4905 2974 3177 6632 3341 6632 1914 8024 852 2769 2361 3307 738 5440 5991 1429 4170 782 4638 1164 4660 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:267]:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:268]:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:269]:label_ids: 11 8 8 8 8 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:262]:*** Example ***\n",
            "I:NER Training:[ber:con:263]:guid: dev-2\n",
            "I:NER Training:[ber:con:265]:tokens: 不 同 层 次 的 人 ， 有 把 吃 绿 色 食 品 放 在 第 一 位 ， 也 有 把 不 挨 饿 放 在 第 一 位 的 ， 但 我 认 为 总 体 的 来 说 把 不 挨 饿 放 在 第 一 位 是 应 该 的 ， 因 为 缺 少 粮 食 而 挨 饿 的 人 太 多 了 。\n",
            "I:NER Training:[ber:con:266]:input_ids: 101 679 1398 2231 3613 4638 782 8024 3300 2828 1391 5344 5682 7608 1501 3123 1762 5018 671 855 8024 738 3300 2828 679 2917 7662 3123 1762 5018 671 855 4638 8024 852 2769 6371 711 2600 860 4638 3341 6432 2828 679 2917 7662 3123 1762 5018 671 855 3221 2418 6421 4638 8024 1728 711 5375 2208 5117 7608 5445 2917 7662 4638 782 1922 1914 749 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:267]:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:268]:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:269]:label_ids: 11 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 6 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:262]:*** Example ***\n",
            "I:NER Training:[ber:con:263]:guid: dev-3\n",
            "I:NER Training:[ber:con:265]:tokens: 人 类 应 该 想 办 法 解 除 这 个 不 益 的 事 。\n",
            "I:NER Training:[ber:con:266]:input_ids: 101 782 5102 2418 6421 2682 1215 3791 6237 7370 6821 702 679 4660 4638 752 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:267]:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:268]:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:269]:label_ids: 11 8 8 8 8 8 8 8 4 9 8 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:262]:*** Example ***\n",
            "I:NER Training:[ber:con:263]:guid: dev-4\n",
            "I:NER Training:[ber:con:265]:tokens: 你 们 应 该 身 体 保 重 ， 那 样 就 你 们 也 许 看 到 我 成 功 的 样 子 ， 而 看 到 我 的 孩 子 。\n",
            "I:NER Training:[ber:con:266]:input_ids: 101 872 812 2418 6421 6716 860 924 7028 8024 6929 3416 2218 872 812 738 6387 4692 1168 2769 2768 1216 4638 3416 2094 8024 5445 4692 1168 2769 4638 2111 2094 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:267]:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:268]:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I:NER Training:[ber:con:269]:label_ids: 11 8 8 8 8 1 5 5 5 8 8 8 1 5 5 5 5 8 8 8 8 8 8 8 8 8 4 7 8 8 8 8 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_base-0.0.9-py3.6.egg/bert_base/train/bert_lstm_ner.py:335: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "I:NER Training:[ber:mod:371]:*** Features ***\n",
            "I:NER Training:[ber:mod:373]:  name = input_ids, shape = (64, 202)\n",
            "I:NER Training:[ber:mod:373]:  name = input_mask, shape = (64, 202)\n",
            "I:NER Training:[ber:mod:373]:  name = label_ids, shape = (64, 202)\n",
            "I:NER Training:[ber:mod:373]:  name = segment_ids, shape = (64, 202)\n",
            "shape of input_ids (64, 202)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_base-0.0.9-py3.6.egg/bert_base/bert/modeling.py:359: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_base-0.0.9-py3.6.egg/bert_base/bert/modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "2020-08-25 14:34:12.625389: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-08-25 14:34:12.676288: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2020-08-25 14:34:12.679288: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1c5b2e0 executing computations on platform Host. Devices:\n",
            "2020-08-25 14:34:12.679359: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF2PHXta-d4k",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}